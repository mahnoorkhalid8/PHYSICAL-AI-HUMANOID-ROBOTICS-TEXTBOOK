---
id: cognitive-planning-llm-to-ros
title: Cognitive Planning: LLM to ROS Actions
---

# Cognitive Planning: LLM to ROS Actions

Translating high-level language commands into robot actions through cognitive planning.

## Introduction to LLM-to-ROS Translation

The integration of Large Language Models (LLMs) with ROS (Robot Operating System) enables natural language interaction with robots. This cognitive planning layer translates high-level human commands into executable robot actions, bridging the gap between human communication and robotic execution.

## Architecture of LLM-ROS Interface

The cognitive planning system consists of several components:
1. **Natural Language Understanding**: Parses human commands
2. **Action Planning**: Maps commands to robot capabilities
3. **ROS Command Generation**: Creates ROS service calls and action requests
4. **Execution Monitoring**: Tracks action completion and handles errors

## Python Code Block: LLM Call Translating Natural Language Command into ROS 2 Actions

```python
import rclpy
from rclpy.node import Node
from std_msgs.msg import String
from geometry_msgs.msg import Pose, Point
from moveit_msgs.srv import GetPositionIK, GetPositionFK
from trajectory_msgs.msg import JointTrajectory, JointTrajectoryPoint
from builtin_interfaces.msg import Duration
import openai
import json
import re
import time

class LLMToROSPlanner(Node):
    def __init__(self):
        super().__init__('llm_to_ros_planner')

        # Publisher for high-level commands
        self.command_publisher = self.create_publisher(String, 'high_level_commands', 10)

        # Publisher for trajectory commands
        self.trajectory_publisher = self.create_publisher(
            JointTrajectory,
            '/joint_trajectory_controller/joint_trajectory',
            10
        )

        # Service clients for MoveIt
        self.ik_client = self.create_client(GetPositionIK, 'compute_ik')
        self.fk_client = self.create_client(GetPositionFK, 'compute_fk')

        # Robot joint names for humanoid
        self.joint_names = [
            'left_hip_joint', 'left_knee_joint', 'left_ankle_joint',
            'right_hip_joint', 'right_knee_joint', 'right_ankle_joint',
            'left_shoulder_joint', 'left_elbow_joint',
            'right_shoulder_joint', 'right_elbow_joint',
            'head_pan_joint', 'head_tilt_joint'
        ]

        # Initialize OpenAI client
        # Note: In a real implementation, you'd set your API key appropriately
        # openai.api_key = "your-api-key-here"

        self.get_logger().info('LLM to ROS Planner initialized')

    def process_natural_language_command(self, command):
        """
        Process a natural language command and translate to ROS actions
        """
        self.get_logger().info(f'Processing command: {command}')

        # First, use LLM to understand and decompose the command
        structured_command = self.llm_understand_command(command)

        # Plan the sequence of ROS actions
        ros_actions = self.plan_ros_actions(structured_command)

        # Execute the actions
        self.execute_ros_actions(ros_actions)

        return ros_actions

    def llm_understand_command(self, command):
        """
        Use LLM to understand the natural language command
        """
        # In a real implementation, this would call an actual LLM API
        # For demonstration, we'll simulate the LLM response

        # Example LLM prompt structure:
        llm_prompt = f"""
        You are a robot command interpreter. Convert the following human command into a structured format.
        Command: "{command}"

        Respond in JSON format with:
        - action_type: The main action to perform
        - target_object: Object to interact with (if any)
        - location: Where to perform the action
        - parameters: Additional parameters

        Example response: {{"action_type": "grasp", "target_object": "red cup", "location": "kitchen table", "parameters": {{"height": 0.5}}}}
        """

        # Simulate LLM response - in real implementation, this would be an API call
        simulated_response = self.simulate_llm_response(command)

        self.get_logger().info(f'LLM response: {simulated_response}')
        return simulated_response

    def simulate_llm_response(self, command):
        """
        Simulate LLM response for demonstration purposes
        """
        # This would be replaced with actual LLM API call in production
        command_lower = command.lower()

        if 'clean' in command_lower or 'room' in command_lower:
            return {
                "action_type": "navigate_and_clean",
                "target_location": "living room",
                "parameters": {"cleaning_tool": "duster", "duration": 300}
            }
        elif 'grasp' in command_lower or 'pick up' in command_lower:
            return {
                "action_type": "grasp_object",
                "target_object": "cup",
                "location": "table",
                "parameters": {"height": 0.8, "gripper_width": 0.05}
            }
        elif 'move' in command_lower or 'go to' in command_lower:
            return {
                "action_type": "navigate",
                "target_location": "kitchen",
                "parameters": {"speed": 0.5}
            }
        elif 'wave' in command_lower or 'greet' in command_lower:
            return {
                "action_type": "gesture",
                "gesture_type": "wave",
                "parameters": {"arm": "right", "repetitions": 3}
            }
        else:
            return {
                "action_type": "unknown",
                "parameters": {}
            }

    def plan_ros_actions(self, structured_command):
        """
        Plan the sequence of ROS actions based on the structured command
        """
        action_type = structured_command.get('action_type', 'unknown')
        actions = []

        if action_type == 'navigate':
            target_location = structured_command.get('target_location', 'unknown')
            location_coords = self.get_location_coordinates(target_location)
            actions.append({
                'type': 'navigation',
                'target_pose': location_coords,
                'parameters': structured_command.get('parameters', {})
            })

        elif action_type == 'grasp_object':
            target_object = structured_command.get('target_object', 'unknown')
            object_pose = self.get_object_pose(target_object)
            actions.extend([
                {
                    'type': 'navigation',
                    'target_pose': self.get_approach_pose(object_pose),
                    'parameters': {}
                },
                {
                    'type': 'grasp',
                    'target_pose': object_pose,
                    'parameters': structured_command.get('parameters', {})
                }
            ])

        elif action_type == 'gesture':
            gesture_type = structured_command.get('gesture_type', 'unknown')
            actions.append({
                'type': 'trajectory',
                'trajectory': self.generate_gesture_trajectory(gesture_type, structured_command.get('parameters', {})),
                'parameters': structured_command.get('parameters', {})
            })

        elif action_type == 'navigate_and_clean':
            target_location = structured_command.get('target_location', 'unknown')
            location_coords = self.get_location_coordinates(target_location)
            actions.extend([
                {
                    'type': 'navigation',
                    'target_pose': location_coords,
                    'parameters': structured_command.get('parameters', {})
                },
                {
                    'type': 'cleaning_pattern',
                    'pattern': 'grid',
                    'parameters': structured_command.get('parameters', {})
                }
            ])

        return actions

    def get_location_coordinates(self, location_name):
        """
        Get coordinates for a named location
        """
        # In a real system, this would come from a map or semantic localization
        locations = {
            'kitchen': {'x': 2.0, 'y': 0.0, 'z': 0.0, 'theta': 0.0},
            'living room': {'x': 0.0, 'y': 2.0, 'z': 0.0, 'theta': 1.57},
            'bedroom': {'x': -2.0, 'y': 0.0, 'z': 0.0, 'theta': 3.14},
            'office': {'x': 0.0, 'y': -2.0, 'z': 0.0, 'theta': -1.57}
        }
        return locations.get(location_name.lower(), {'x': 0.0, 'y': 0.0, 'z': 0.0, 'theta': 0.0})

    def get_object_pose(self, object_name):
        """
        Get pose for a named object
        """
        # In a real system, this would come from perception
        return {'x': 1.0, 'y': 0.5, 'z': 0.8, 'qx': 0.0, 'qy': 0.0, 'qz': 0.0, 'qw': 1.0}

    def get_approach_pose(self, object_pose):
        """
        Get a safe approach pose near an object
        """
        approach = object_pose.copy()
        approach['x'] -= 0.3  # Approach from front
        return approach

    def generate_gesture_trajectory(self, gesture_type, params):
        """
        Generate trajectory for a specific gesture
        """
        if gesture_type == 'wave':
            return self.generate_wave_trajectory(params)
        elif gesture_type == 'point':
            return self.generate_point_trajectory(params)
        else:
            return self.generate_default_trajectory()

    def generate_wave_trajectory(self, params):
        """
        Generate trajectory for waving gesture
        """
        repetitions = params.get('repetitions', 3)
        arm = params.get('arm', 'right')

        # Define key poses for waving
        if arm == 'right':
            joint_indices = [8, 9]  # right shoulder and elbow
        else:
            joint_indices = [6, 7]  # left shoulder and elbow

        trajectory_points = []
        time_from_start = 0.0

        for i in range(repetitions * 2):  # Each wave has up and down motion
            point = JointTrajectoryPoint()

            # Create wave motion
            if i % 2 == 0:  # Up motion
                positions = [0.5, -0.5]  # Shoulder up, elbow bent
            else:  # Down motion
                positions = [0.0, 0.0]   # Return to neutral

            # Extend to full joint array
            full_positions = [0.0] * len(self.joint_names)
            for idx, pos in enumerate(positions):
                full_positions[joint_indices[idx]] = pos

            point.positions = full_positions
            point.velocities = [0.0] * len(self.joint_names)  # Zero velocity
            point.accelerations = [0.0] * len(self.joint_names)  # Zero acceleration

            time_from_start += 0.5  # 0.5 seconds per phase
            point.time_from_start = Duration(sec=int(time_from_start), nanosec=int((time_from_start % 1) * 1e9))

            trajectory_points.append(point)

        trajectory = JointTrajectory()
        trajectory.joint_names = self.joint_names
        trajectory.points = trajectory_points

        return trajectory

    def generate_default_trajectory(self):
        """
        Generate a default trajectory (home position)
        """
        trajectory = JointTrajectory()
        trajectory.joint_names = self.joint_names
        trajectory.points = []

        point = JointTrajectoryPoint()
        point.positions = [0.0] * len(self.joint_names)  # Home position
        point.velocities = [0.0] * len(self.joint_names)
        point.accelerations = [0.0] * len(self.joint_names)
        point.time_from_start = Duration(sec=2, nanosec=0)

        trajectory.points = [point]
        return trajectory

    def execute_ros_actions(self, actions):
        """
        Execute the planned ROS actions
        """
        for action in actions:
            action_type = action['type']

            if action_type == 'navigation':
                self.execute_navigation_action(action)
            elif action_type == 'grasp':
                self.execute_grasp_action(action)
            elif action_type == 'trajectory':
                self.execute_trajectory_action(action)
            elif action_type == 'cleaning_pattern':
                self.execute_cleaning_action(action)

            # Small delay between actions
            time.sleep(0.1)

    def execute_navigation_action(self, action):
        """
        Execute navigation action
        """
        target_pose = action['target_pose']
        self.get_logger().info(f'Navigating to: {target_pose}')

        # In a real implementation, this would call navigation2
        # For now, we'll just log the action
        nav_command = String()
        nav_command.data = f"navigate_to {target_pose['x']} {target_pose['y']} {target_pose['theta']}"
        self.command_publisher.publish(nav_command)

    def execute_grasp_action(self, action):
        """
        Execute grasp action
        """
        target_pose = action['target_pose']
        params = action['parameters']
        self.get_logger().info(f'Grasping object at: {target_pose}')

        # Generate trajectory to grasp position
        grasp_trajectory = self.generate_grasp_trajectory(target_pose, params)
        self.trajectory_publisher.publish(grasp_trajectory)

    def generate_grasp_trajectory(self, target_pose, params):
        """
        Generate trajectory for grasping
        """
        trajectory = JointTrajectory()
        trajectory.joint_names = self.joint_names
        trajectory.points = []

        # Pre-grasp position (approach)
        pre_grasp_point = JointTrajectoryPoint()
        pre_grasp_positions = [0.0] * len(self.joint_names)
        # Set arm joints to approach position (simplified)
        pre_grasp_positions[6] = 0.5  # Left shoulder
        pre_grasp_positions[7] = 0.2  # Left elbow
        pre_grasp_point.positions = pre_grasp_positions
        pre_grasp_point.velocities = [0.0] * len(self.joint_names)
        pre_grasp_point.accelerations = [0.0] * len(self.joint_names)
        pre_grasp_point.time_from_start = Duration(sec=1, nanosec=0)

        # Grasp position
        grasp_point = JointTrajectoryPoint()
        grasp_positions = pre_grasp_positions.copy()
        grasp_positions[7] = 0.8  # Elbow more bent for grasp
        grasp_point.positions = grasp_positions
        grasp_point.velocities = [0.0] * len(self.joint_names)
        grasp_point.accelerations = [0.0] * len(self.joint_names)
        grasp_point.time_from_start = Duration(sec=2, nanosec=0)

        trajectory.points = [pre_grasp_point, grasp_point]
        return trajectory

    def execute_trajectory_action(self, action):
        """
        Execute trajectory action
        """
        trajectory = action['trajectory']
        self.get_logger().info('Executing trajectory action')
        self.trajectory_publisher.publish(trajectory)

    def execute_cleaning_action(self, action):
        """
        Execute cleaning action
        """
        pattern = action['pattern']
        params = action['parameters']
        self.get_logger().info(f'Cleaning with {pattern} pattern for {params.get("duration", 300)} seconds')

        # Publish cleaning command
        clean_command = String()
        clean_command.data = f"clean_{pattern}_for_{params.get('duration', 300')}_seconds"
        self.command_publisher.publish(clean_command)

def main(args=None):
    rclpy.init(args=args)

    planner = LLMToROSPlanner()

    # Example usage: Process a natural language command
    test_commands = [
        "Please go to the kitchen and bring me a cup of water",
        "Wave hello to the person in front of you",
        "Clean the living room with the duster",
        "Pick up the red cup from the table"
    ]

    for command in test_commands:
        planner.get_logger().info(f"Processing: {command}")
        actions = planner.process_natural_language_command(command)
        planner.get_logger().info(f"Planned actions: {len(actions)}")

        # Small delay between commands
        time.sleep(1)

    # Keep the node alive to continue processing commands
    try:
        rclpy.spin(planner)
    except KeyboardInterrupt:
        pass
    finally:
        planner.destroy_node()
        rclpy.shutdown()

if __name__ == '__main__':
    main()
```

## Advanced LLM Integration Patterns

### Context-Aware Command Processing
```python
class ContextAwareLLMPlanner(LLMToROSPlanner):
    def __init__(self):
        super().__init__()
        self.context_history = []
        self.robot_state = {}
        self.environment_map = {}

    def process_command_with_context(self, command):
        """
        Process command considering historical context and current state
        """
        # Build context from history and current state
        context = self.build_context(command)

        # Enhanced LLM prompt with context
        contextual_prompt = f"""
        Context: {context}

        Human Command: "{command}"

        Based on the context, convert this command to structured robot actions.
        Consider:
        - Robot's current state and location
        - Recent interactions
        - Environmental constraints
        - Task dependencies

        Respond in JSON format with detailed action sequence.
        """

        # Process with LLM (simulated here)
        structured_response = self.simulate_contextual_llm_response(command, context)

        return structured_response

    def build_context(self, new_command):
        """
        Build contextual information for the LLM
        """
        context = {
            'current_time': time.time(),
            'robot_location': self.robot_state.get('location', 'unknown'),
            'battery_level': self.robot_state.get('battery', 100),
            'recent_actions': self.context_history[-5:],  # Last 5 actions
            'environment_objects': list(self.environment_map.keys()),
            'new_command': new_command
        }

        return json.dumps(context, indent=2)
```

## Error Handling and Recovery

Robust LLM-ROS integration requires handling various failure modes:

1. **LLM Misinterpretation**: Verify and validate LLM outputs
2. **ROS Execution Failures**: Implement fallback behaviors
3. **Safety Constraints**: Ensure all actions are safe before execution
4. **Communication Failures**: Handle network and API outages gracefully

## Cross-Module References

For more information about related topics, see:
- Module 1: Bridging Python Agents with RCLPY for ROS integration
- Module 1: ROS 2 Architecture for Humanoids for communication patterns
- Module 1: ROS 2 Nodes, Topics, and Services for messaging systems
- Module 2: Gazebo Simulation Setup for testing cognitive planning in simulation
- Module 2: Physics and Sensor Simulation for environment modeling
- Module 3: Perception and Manipulation for object interaction
- Module 3: Reinforcement Learning Control for adaptive behavior planning
- Module 3: Isaac SDK and Sim Overview for advanced simulation integration
- Module 4: GPT Integration for Conversational AI
- Module 4: Speech Recognition with Whisper for voice command processing
- Module 4: Capstone Project: Autonomous Humanoid for complete system integration
